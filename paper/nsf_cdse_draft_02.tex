\documentclass[preprint,11pt,authoryear]{elsarticle}
\usepackage{amsmath}
\usepackage[font=sf, labelfont={sf,bf}, margin=0cm]{caption}
\usepackage[inline]{enumitem}
\usepackage{epstopdf}   
\usepackage[margin=1in]{geometry}
\usepackage{lineno}
\usepackage{placeins}
\usepackage{subcaption}
\usepackage{color}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{tabulary}
%\usepackage[section]{placeins} %prevents floats from appearing before their sections. web address  https://tex.stackexchange.com/questions/32598/force-latex-image-to-appear-in-the-section-in-which-its-declared
\usepackage{float}

\journal{Chemical Engineering Research and Design}
\begin{document}
\begin{frontmatter}
\title{ Efficient uni-directional PBM-DEM coupling using parallelization techniques on HPCs.}
\author{A1}
\author{A2}
\author{A3}
\author{A4}
\author{A5} 
\ead{A5@a5.com}
\cortext[cor1]{Corresponding author}
\address{Department of Chemical and Biochemical Engineering, Rutgers, The State University of New Jersey, Piscataway, NJ, USA 08854}
\begin{abstract}
abstract text goes here ...
\end{abstract}
\begin{keyword}
Population balance model \sep Granulation \sep Discrete element methods  \sep MPI and OpenMP \sep Pharamcuetical process design
\end{keyword}
\end{frontmatter}
\linenumbers

\section{Introduction \& Objectives} 

\par Half of all industrial chemical production is carried out using particulate processes (\cite{seville1997}). Important products that are produced using particulate processes include detergents, aerosols, fertilizers, and pharmaceuticals. Particulate processes have complex phenomena due to the chaotic microscale behaviour of individual particles in the system that determine the bulk behaviour of the system. Due to the complex nature of these processes there is a lack of governing equations which can accurately describe them (\cite{sen2013}). As a result industries such as pharmaceuticals use expensive heuristic studies and inefficient operational protocols with high recycle ratios to meet strict regulatory standards (\cite{Ramachandran2009}). In an effort to cut costs and improve efficiencies there is a high demand for accurate models that can be used for process design and control systems.
\par Wet granulation is a key process in the manufactuing of tablets, where fine powders are converted to larger granules using a liquid binder. The three processes influencing granulation are wetting and nucleation, consolidation and aggregation and attrition and breakage\citep{Iveson2001}\citep{Cameron2005}. As the liquid is added to the fine powder, it forms a porous nuclei that can coalesce, deform and break\citep{Barrasso2015ces}. There is an alteraion in the properties of these nuclie as they can take up additional liquid or finer powder.
\par  Normally, the granulation process models can be differentiated into two distinct categories viz. process models and particle-scale models\citep{Barrasso2015ces}. Discrete element method (DEM)(\cite{Cundall1979}) is a particle scale model that has proven to be an accurate way to model a bulk material using microscale particle level interactions.  DEM does not take into account the changes in particle size, thus neglecting the particle size dependent properties like aggregation, breakage, consolidation and liquid addition\citep{Barrasso2015ces}.The downside to this model is that they are computationally heavy since it calculates force on each particle present in the simulation. Population Balance Model (PBM) is a process model which is used to model the granulation process. It takes into account the changes in particle size and the property related mechanisms. This model lacks sensitivty to process paramters and the equipment geometry. Thus, to complement the limitations of each of these modelling methods they are coupled to provide a more accurate model. Initally, \cite{ingram2005} coupled PBM and DEM using two different mutliscale frameworks and focussed on methods on integrating the two methods and information exchange required between the two. Later efforts on coupling of PBM and DEM have been uni-directional in nature, where the collision data is obtained from the DEM and then have been used in PBM. \cite{gantt2006} used the DEM data to build mechanistic model for the PBM. \cite{Goldschmidt2003} solved a PBM using DEM by replacing smaller particles as they sucessfully coalesce by larger particles. A mechanistic aggregation model was developed for wet granulation by \cite{Reinhold2012}. A hybrid model for one-way coupling has been reported for continuous mixing (\cite{sen2013} \& \cite{sen2013b}) and is discussed in later sections. 
A similar uni-directional approach is followed in this work. \cite{Barrasso2015ces} developed a novel algorithm for bi-directional coupling of PBM and DEM using a mechanistic kernel and \citep{Barrasso2015cerd} also performed a bi-directional coupling of PBM and DEM which included the effect of collsions on the PBM.

%\par Use of parallel techniques by scientists have helped reduce the amount of time required to solve computationally heavy tasks into a array of smaller and more manageable tasks, which are solved simultaneously in various smaller processes. 
\par Some of the potential benefits of using HPCs in the pharmaceutical industry include really high accuracy for parameter estimation as detialed particle-scale simulations can be performed quickly. On-line modelling of continuous processes as dicussed by \citep{Bettencourt2017}, as they performed the simulations more than two times faster than the actual granulation time. Such quick simulations can also help improve control of continuous pharmacuetical processes. Modelling these processes also abides to the Quality-by-Design(QbD) approach being employed by the industry. This work showcases the speed improvements that can be achieved using HPCs and how they can help improve process design.

\subsection{Objectives}
\par The main objective of this study was to develop new techniques to parallelize our model and use high perfomance computing (HPC) equipment in order to reduce the computation times for the linked DEM-PBM simulations. Specific objectives of each individual study are listed below: 
\begin{itemize}
\item Developed a 4-Dimensional PBM that is parallelized using hybrid techniques (MPI + OpenMP)  for optimal utilization of modern high performance computing equipment.
\item LIGGGHTS (MPI-only implementation) has been used to perfrom Discrete Element Model (DEM) simulations to  model the micromechanics of the Lodgitech granulator. 
\item These 2 techniques were linked uni-directionally to provide an accurate model of the granulator.	    
\end{itemize}



\section{Background \& Motivation}

%\subsection{Particulate Processes}
\par Particulate processes are ones in which a system of discrete species exist, such as granules or catalyst pellets, that undergo changes in average composition, size, or other pertinent properties. Such processes are prevalent in the pharmaceutical industry. These processes have great oppurtunities in better equipment design, process efficiency and scale-up (\cite{Ketterhagen2009}). 

%\par as result use time consuming and expensive heuristics or inefficient operation protocol  dotdotdot also reason most blah done in batch mode still bec continuous mode compounds complexity of these systems \par will need to switch to continuous etc  
%\par segway with need to better understand these systems from a modelling/theory point of view dotdotdot QbD etc 

\subsection{Modeling}
\par The quality by design (QbD) concept i.e, theoritically modelling the process for better quality of the product is being implemented by the pharmaceutical industry due to its cost benefits. Also, the paradigm shift of the industry towards continuous manufacturing, emphasizes the need for a more accurate model. This further helps develop better control stratergies for the process. The modelling of particulate processes is more time consuming and computationally expensive when compared to a fluid systems since the particles are considered as individual entities rather than a continuum like in fluid systems. The models discussed ahead represent the particle-particle interaction at meso and micro-scale.	  
\subsubsection{Discrete Element Modeling (DEM)}
\par Discrete Element Method is a simulation technique used to monitor the behaviour of each particle as a separate entity compared to other bulk continuum models. This method tracks the movement of each of the particles with in the space, records the collisions of each particle with the geometry as well as with each other and it is also subject to other force fields like gravity (\cite{Barrasso2015cerd})  This model is based on the Newton's laws of the motion and is expressed as in Equations \ref{eqn:bkgd_dem_n2law} and  \ref{eqn:bkgd_dem_forcebal} : \\
\begin{align}
m_i\frac{dv_i}{dt} &= F_{i,net} \label{eqn:bkgd_dem_n2law} \\
F_{i,net} &=  F_{i,coll} +  F_{i,ext} \label{eqn:bkgd_dem_forcebal}
\end{align}
\par  In the above equations $m_i$ represents the mass of the particle, $v_i$ represents the velocity of the particle, $F_{i,net}$  represents the net force on the particle, forces on the particle due to collisions and other external forces are represented in $F_{i,coll}$ and $F_{i,ext}$ respectively.
\par The distance between each particle calculated at every time step and if the distance between two particles is less than the sum of the radii (for spherical particles)  a collision between the two particles is recorded. The tolerance for overlap is low in the normal as well as the tangential direction. Microscale DEM simulations are computationally demanding and simulations may take upto several days to replicate a few seconds of real time experiments. Many methods have been implemented to increase the speed of these simulations, such as scaling by increasing the size of the particles. These approximations are good in understanding the physics of the system but are not directly applicable to process-level simulations. 
%\par Commercially available softwares for DEM simualtions have not been able to take advantage of the recent advances in computing power in terms of supercomputers. This
\par Thus, this method for granular powder is usually replaced by Population Balance Method which is a much quicker approximation as it is a bulk model.  

\subsubsection{PBM}
\par Population balance models (PBM) predict how groups of discrete entities will behave on a bulk scale due to certain effects acting on the population with respect to time (\cite{ramkrishna2014}). In the context of process engineering and granulation, population balance models are used to describe how the number densities, of different types of particles, in the granulator change as rate processes such as aggregation and breakage reshape particles (\cite{Barrasso2013}). A general form of population balance model is shown here as equation \ref{eqn:bkgd_pbm_general}.

\begin{align}
\frac{ \partial}{\partial t}F(\textbf{v},\textbf{x},t) +& \frac{\partial}{\partial \textbf{v}}[F(\textbf{v},\textbf{x},t)\frac{d\textbf{v}}{dt}(\textbf{v},\textbf{x},t)] + \frac{\partial}{\partial \textbf{x}}[F(\textbf{v},\textbf{x},t)\frac{d\textbf{x}}{dt}(\textbf{v},\textbf{x},t)] \notag\\
&= \Re_{formation}(\textbf{v},\textbf{x},t)+\Re_{depletion}(\textbf{v},\textbf{x},t)+\dot{F}_{in}(\textbf{v},\textbf{x},t)-\dot{F}_{out}(\textbf{v},\textbf{x},t)
\label{eqn:bkgd_pbm_general} 
\end{align}

\par In equation (\ref{eqn:bkgd_pbm_general}), $\textbf{v}$ is a vector of internal coordinates. For modelling a granulation process $\textbf{v}$ is commonly used to describe the solid, liquid, and gas content of each type of particle. The vector $\textbf{x}$ represents external coordinates, usually spatial variance. For a granulation process this  account for spatial variance in the particles as they flow along the granulator.




\subsubsection{Multi-physics models}
\par The use of multi-physics models has recently been adapted to understand the behaviour of particle systems. These models help understand the physics of the system at various scales \textit{i\.e\.} micro, meso and macro scale(\cite{sen2014}). Particle process dynamics have been inferred from coupling of various physics models \textit{viz.} Computational Fluid Dynamics (CFD), DEM and PBM. Earlier works from \cite{sen2014} and \cite{Barrasso2015cerd} have successfully predicted process dynamics of the granulation process using such multi-physics models.
\par In this work, a coupling of DEM and PBM has been implemented. The PBM gives meso scale information while the DEM gives particle scale information. Combination of these two methods help comprehend the process dynamics with more accuracy. The calculations involved due to the number of particles involved in the DEM process as well as PBM become very computationally heavy. The recent development in the design of CPUs and increasing number of cores in the CPU, it makes sense to utilize them to make the simulations faster. Thus, implementation of various parallel computing techniques was employed in this work to help improve the simulation times.  


\subsection{Parallel Computing and Computer Architectures}
%\subsubsection{Overview}
\par The goal of parallel computing is to distribute large amounts of computation across many compute cores to solve problems faster (\cite{wilkinson2005}).
\subsubsection{Computer Architecture}
\par Parallel programs are commonly run on computer clusters. Computer clusters are a collection of nodes interconnected by a high speed communication network for message passing from one node to another. Analogous to a conventional PC each node as one or more CPUs and RAM. Commonly nodes are manufactured with two CPUs, each CPU is a multi-core meaning it has multiple compute cores that each can carry out calculations separately from one another. CPUs also have built in memory called cache that is much faster than RAM which is why for optimal performance cache utilization should be favoured over RAM when possible. On a node memory is divided by CPU sockets, so each CPU has direct access to memory local to its own socket, but accessing memory on another socket is much slower \cite{Jin2011}. For this reason data that is needed for computation should be stored locally to the CPU that needs it.  
\par Two common classes of computer architecture classified by memory locality features are distributed memory systems and shared memory systems. These two classes co-exist in a cluster, thus providing the benfits of each. All the nodes share memory using explicit message passing while each has its own independent memory. The cores on each node can access data from the shared memory without any explicit message passing statements from the user. While designing a parallel program all these aspects need to be considered for optimal perfomance of the code (\cite{Adhianto2007}).
%Computer architectures are often classified by memory locality features. There are two distinct classes, distributed memory systems and shared memory systems. A cluster is a combination of the two classes. Each node operates its memory independently of the other nodes and explicit message passing is needed to share memory between nodes. While the cores on a node can operate in shared memory mode since memory updates can be made without explicit message passing statements from the user. All of these aspects of the computer architecture should be considered when designing a parallel program for the best performance  \textcolor{cyan}{maybe reword better to make more like CACE paper}.

\subsubsection{Parallel Application Program Interfaces}
\par Message Passing Interface (MPI) is a common parallel computing application interface standard. MPI is used for distributed memory parallel computing, this is because MPI will operate every MPI process as a discrete unit that does not share memory with the other processes unless explicit message passing is used. Even on shared a single node where the hardware is supports shared memory computing, MPI will still operate it in a distributed memory fashion \cite{Jin2011}. Operating all cores as distinct units also means they each need their own copy of all variables used for computation which results in a large overall memory foot print compared to a same system if it was operated in shared memory. 
\par Open Multi-Processing (OMP) is another application program interface stand for parallel computing. OMP is used for shared memory and can take advantage of shared memory systems which can result in much faster computation. It does not work well on distributed systems though. This prevents it from being used to efficiently carry out computations across multiple nodes of a cluster simultaneously \cite{Jin2011}. 
\par Since MPI is preferred for distributed computing and OMP is better for shared computing many individuals have studied the performance of MPI vs MPI+OMP methods and many studies have used MPI+OMP for scientific computation for improved performance. Often times a trade of is made between optimizing a program for performance and trying to make it flexible enough to run on many different computer architectures. In the conclusion to the work by \cite{Bettencourt2017} it was found that hybrid methods for PBMs allow the code flexibility for different architectures while still maintaining good performance.  It was also reported that only the external(spatial) coordinates of the PBM were parallelized. In this current work external and internal(compositions) calculations are parallelized.% \textcolor{cyan}{comment about how int and ext pll methods means better model for xyz reasons dotdotdot helps to explicitly say reason} 
\subsubsection{Previous works on parrallel PBM and DEM}
\par  The idea of parallelization to reduce the amount of time required has been employed by various researchers in the past. \cite{Gunawan2008} used high-resolution finite volumes solution methods for the parallelization of their PBM. \cite{Gunawan2008} performed load balancing effectively by decomposing the internal coordinates of their PBM. They acheived speed improvements upto 100 cores on one system size, but was not tested for models with more dimensions. Moreover, they mentioned that the parallelisation could be improved using shared memory processing. \cite{Bettencourt2017} took a hybrid approach towards the parallelization of the PBM using both Message Parsing Interface (MPI) and Open Multi-Processing. The hybrid parallelisation helped achieve a speed up of about 98\% over the serial code. \cite{Prakash2013a},\cite{Prakash2013b} used the inbuilt Parallel Computation Toolbox (PCT) in Matlab to parallelize their PBM on lower number of cores, but this faced the shortcomings of Matlab's internal processing and could not achieve the speed improvements of parallelistion of a program if it were written in a native programming language like C or FORTRAN. 
\par  LIGGGHTS is an open-source software used to perform DEM simulations. This package natively supports MPI for parallelizing the simulation by static decomposition which partitions space such that the area of communication between the the MPI ranks is minimized. \cite{kacianauskas2010} used load balancing methods similar to a static decompostion and observed that this works well for a mono-dispersed system but the computational effort increases for simulations for poly-dispersed material. \cite{Gopalakrishnan2013} also reported a speed increase and a parallel efficiency of about 81\% in their CFD-DEM simulation. LIGGGHTS could not take advantage of shared memory interfaces since it did not support OpenMP. \cite{Berger2015} implemented hybrid parallelization methods for the particle-particle interaction and the particle-wall interaction modules in LIGGGHTS and also used the Zoltan library \citep{Boman2012} developed by Sandia National Laboratories for dynamic load balancing. They achieved a speed improvement of about 44\% for simulations performed on higher number of cores, but there was no significant speed improvement for smaller core counts. 


\section{Methods}

\subsection{DEM}
\subsubsection{LIGGGHTS}
\par LAMMPS Improved for general granular and granular heat transfer simulations (LIGGGHTS v3.60) (\cite{Kloss2012}) developed by DCS computing was used for all the simulation performed in this study. Edits were made to the compute\_contact\_atom source file to obtain particle $-$ particle collisions. The aforementioned version of LIGGGHTS was compiled using the mvapich (mvapich2 v2.1) and intel (intel v15.0.2) compilers with the -o3 optimisation option as well as an option to sideload the process to the Xeon phi co-processors was added. The studies were performed on STAMPEDE supercomputer located at Univeristy of Texas, Austin. The hardware configuration of each node consists of 2 8-core Intel Xeon E5-2680 processors based on the Sandy Bridge architecture, 32 gb of memory with QPI interconnects at 8.0 GT/s PCI-e lanes.


\subsubsection{Geometry}    
%\par \textcolor{red}{check other file Charles uploaded to see if that one was more "journal ready"}

\par In this study, the L\"{o}dige CoriMix CM5 continuous high shear granulator has been studied. Its geometry was developed using the SolidWorks$^{TM}$ (Dassault Syst\`{e}mes). This granulator consisted of a high speed rotating element enclosed within a horizontal cylindrical casing. The casing (shown in Figure \ref{fig:mthdsDemCharlesGranShell}) consists of a cylinder with diameter of 120 mm at the inlet and 130 mm at the outlet and having a total length of 440 mm. A vertical inlet port is provided at one end of the casing and an angled outlet port is provided at the larger end of the case. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.2]{shell_final_pic.pdf}
\caption{ Shows an isometric view of the granulator casing.}
%\caption{hello yuktesh}
\label{fig:mthdsDemCharlesGranShell}
\end{figure}

\par  The impeller consists of a cylindrical shaft of length 370 mm and diameter 68 mm with four flattened sides 15 mm wide running along the axis. The blades are placed on these flattened sides as shown in figure \ref{fig:mthds_dem_charles_impeller}. There are three different blade elements on the shaft (figure \ref{fig:mthds_dem_charles_impeller}). At the granulator inlet, there are 4 paddle shaped feed elements following which there are 20 tear drop shaped shearing elements  and finally 4 trapezoidal blades near the exit. All these elements are placed in a spiral configuration. The final configuration of the granulator is shown in figure \ref{fig:mthds_dem_charles_fig5pt3and4_blades_n_isometric}.

\begin{figure}[H]
\centering
\includegraphics[scale=0.2]{impeller_final_pic.pdf}
\caption{a) Shows the side view of the impeller. b) and c) show the front and back views of the impeller which correspond to the inlet and outlet ends respectively.}
%\caption{hello yuk}
\label{fig:mthds_dem_charles_impeller}
\end{figure}    

\begin{figure}[H]

\begin{subfigure}{.3\textwidth}
	\centering
	\includegraphics[scale=0.075]{feed_element.pdf}	      
	\caption{Feed element}
	\label{fig:mthds_dem_feed_element}
\end{subfigure}%
\begin{subfigure}{.3\textwidth}
	\centering
\includegraphics[scale=0.075]{shear_element.pdf}
	\caption{Shear element}
	\label{fig:mthds_dem_shear_element}
\end{subfigure}
	\begin{subfigure}{.3\textwidth}
	\centering
\includegraphics[scale=0.075]{exit_element.pdf}
	\caption{Exit element}
	\label{fig:mthds_dem_exit_element}
\end{subfigure}



\caption{ Various components of the impeller}
%\caption{hello tesh}
\label{fig:mthds_dem_charles_fig5pt3and4_blades_n_isometric}
\end{figure}     




\subsubsection{Meshing}
\par After the geometry was built in SolidWorks$^{TM}$ (Dassault Syst\`{e}mes) the shell and impeller were exported as STL files. The coarsest output option was used to keep the STL files small and simple for faster computations times. They were also exported not keeping there original coordinates  This resulted in the impeller having 2802 faces and 1281 points with approximately a file size of 775 KBs. The shell had 1948 faces and 720 points and size was about 544 KBs.  
\par Meshlab was used to align the STL files for importing into LIGGGHTS. No mesh treatments were used on the STLs. 
\par The meshes were then imported into LIGGGHTS using the write command in serial. This resulted in 50 elements of the impeller file having "highly skewed elements", which have more than 5 neighbours per surface or have an angle less than 0.0181185 degrees, that according to LIGGGHTS would degrade parallel performance. The write exclusion list command in LIGGGHTS was used and this exclusion list file as then used in the simulation to skip the highly skewed elements during the simulation 
%The shell did not have any skewed elements \textcolor{cyan}{(FUTURE SOLUTION? - perhaps we can use the output from liggghts exclusion list to find exact elements of issue. then we can use meshlab to exclude those peices or remesh thos individual pieces into better shapes with less skewed elements. might be better for a letter paper though}

\subsubsection{DEM input file settings}
\par The DEM simulation in LIGGGHTS are setup using an input script which defines the physical parameters of the particles, importing of the geometry, particle insertion commands, various physics models to be used during the simulation as well as various compute and dump commands to help print the data required for post-processing of the data. The particles were considered to be granular in nature. The Hertzian model was used for non-adhesive elastic contact between the granular particles. The particles were inserted inside the granluator from the inlet at a constant mass flow rate of 15 kilgrams per hour. The rotation speed of the impeller was kept throughout the study at 2000 rotations per minute. Such a high rotation speed was chosens since this would lead to high shear between the particles and the walls of the shell resulting in better size control of the granules. There were 2 sets of simulations that were performed, one with mono-sized partcles and second consisting of a distribution of sizes. The particle radii chosen for mono-sized simulation varied 0.59mm - 2mm, consecutive particles radii had volume twice of one before them. The radii range of the distributed size simulation was 1mm - 3mm. The difference in the mechanics of these two simulations is dicussed later in the results section. The physical constants used for the simulations are given in Table \ref{table:mthds_dem_input}.
\par The simulation data was collected after a constant number of iterations for the visulation of the particles inside the shell, further post processing . The collisions between each of the particles and the collisions between of the particle and the geometry was collected and used in the PBM. 

\begin{table}[!htb]%[H]
\caption{Physical Properties of the particle for the LIGGGHTS input script} \label{table:mthds_dem_input}
\begin{center}
\begin{tabular}{l|c|c}
\hline
\bf{Parameter} &\bf{Value} &\bf{Units}\\
\hline
Young's Modulus of particles  & $8 \times 10^{6}$ & $N.m^{-2}$\\
Young's Modulus of Geometry  & $1 \times 10^{9}$ & $N.m^{-2}$\\
Poisson's Ratio & $0.2$ & $-$\\
Coefficient of restitution (constant for all collisions) & $0.4$ & $-$\\
Coefficient of static friction & $0.5$ & $-$\\
Coefficient of rolling friction  & $0.2$ & $-$\\
Density of the granules & $500$ & $kg.m^{-3}$\\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{DEM data post processing}
\par The post processing of the data obtained from the DEM simulations was done using MATLAB. The first test run on the output data was to determine if the simulation had reached steady-state. The mass inside the granulator was found out by averaging it over 5 time steps and then compared to mass inside the granulator after every 10000 time steps (about $5$x$10^{-4}$ seconds) with a tolerance of about 10\%. If the mass was found to be constant for most of the iterations, it was considered to be at steady state. Another test to determine steady state was to monitor the number of collision inside the granulator. The visualisation of the simulation data was done by running the LIGGHTS post processing (LPP) script over the dump files to convert the dump files to .stl files. These files were then loaded in to Paraview\citep{henderson2004} for a graphical representation of the simulation. It can be seen that the number of collision start to oscillate around a mean value. The number of collisions were then plotted and steady state time was determined.
\par A precautionary script was also run so as to determine that no particles were lost due to overlap of the geometry with the particles as well as from particle particle overlap.


\subsection{PBM}
\subsubsection{Model Development}
\par The PBM equation developed for this work can be expressed as shown below:
\begin{align}
\frac{d}{dt}F(s_1,s_2,x)=\Re_{agg}(s_1,s_2,x)+R_{break}(s_1,s_2,x)+\dot{F}_{in}(s_1,s_2,x)-\dot{F}_{out}(s_1,s_2,x)
\label{eqn:mthds_pbm_overall} 
\end{align}
%	    \par \textcolor{red}{citation?}    

\par where, $F(s_1,s_2,x)$ is the number of particles with an API volume of $s_1$ and an excipient volume of $s_2$ in the spatial compartment $x$. The rate of change of number of particles with time in different size classes depend on the rate of aggregation $\Re_{agg}(s_1,s_2,x)$ and the rate of breakage $\Re_{break}(s_1,s_2,x)$. Also, the rate of particles coming into, $F_{in}(s_1,s_2,x)$ and going out, $F_{out}(s_1,s_2,x)$ of the spatial compartment due to particle transfer affect the number of particles in different size classes. 
The rate of change of liquid volume is calculated using the equation: 

\begin{align}
\frac{d}{dt}F(s_1,s_2,x)l(s_1,s_2,x)&= \Re_{liq,agg}(s_1,s_2,x)+\Re_{liq,break}(s_1,s_2,x)+\dot{F}_{in}(s_1,s_2,x)l_{in}(s_1,s_2,x)\notag\\
&-\dot{F}_{out}(s_1,s_2,x)l_{out}(s_1,s_2,x)+F(s_1,s_2,x)\dot{l}_{add}(s_1,s_2,x)
\end{align}

\par where, $l(s_1,s_2,x)$ is the amount of liquid volume in each particle with API volume of $s_1$ and excipient volume of $s_2$ in the spatial compartment $x$. $\Re_{liq,agg}(s_1,s_2,x)$ and $\Re_{liq,break}(s_1,s_2,x)$ are respectively the rates of liquid transferred between size classed due to aggregation and breakage. $l_{in}(s_1,s_2,x)$ and $l_{out}(s_1,s_2,x)$ are respectively the liquid volumes of the particles coming in and going out of the spatial compartment. $l_{add}(s_1,s_2,x)$ is the volume of liquid acquired by each particle in the compartment at every time step due to external liquid addition. 
\par Similarly, the rate of change of gas volume is calculated using the following equation: 
\begin{align}
\frac{d}{dt}F(s_1,s_2,x)g(s_1,s_2,x)&= \Re_{gas,agg}(s_1,s_2,x)+\Re_{gas,break}(s_1,s_2,x)+\dot{F}_{in}(s_1,s_2,x)g_{in}(s_1,s_2,x)\notag\\
&-\dot{F}_{out}(s_1,s_2,x)g_{out}(s_1,s_2,x)+F(s_1,s_2,x)\dot{g}_{cons}(s_1,s_2,x)
\end{align}
%	    \par \textcolor{red}{citation?}

\par where, $g(s_1,s_2,x)$ is the gas volume of each particle with API volume of $s_1$ and excipient volume of $s_2$ in the spatial compartment $x$. $\Re_{gas,agg}(s_1,s_2,x)$ and $\Re_{gas,break}(s_1,s_2,x)$ are respectively the rates of gas transferred between size classed due to aggregation and breakage. $g_{in}(s_1,s_2,x)$ and $g_{out}(s_1,s_2,x)$ are respectively the gas volume of the particles entering and leaving the spatial compartment. $g_{cons}(s_1,s_2,x)$ is the volume of gas coming out of each particle in the compartment at every time-step due to consolidation of the particles. 
\par The rate of aggregation, $\Re_{agg}(s_1,s_2,x)$ in Equation \ref{eqn:mthds_pbm_overall} is calculated as  
\begin{align}
\Re_{agg}(s_1,s_2,x)&= \frac{1}{2}\int _0^{s_1} \int_0^{s_2} \beta(s_1',s_2',s_1-s_1',s_2-s_2',x)F(s_1',s_2',x)F(s_1-s_1',s_2-s_2',x)ds_1'ds_2'\notag\\ 
&- F(s_1,s_2,x)\int _0^{s_{max_1}-s_1} \int_0^{s_{max_2}-s_2} \beta(s_1,s_2,s_1',s_2',x)F(s_1',s_2',x)ds_1'ds_2'
\end{align}
%	     \par \textcolor{red}{citation?}

\par where, the aggregation kernel, $\beta(s_1,s_2, s_1',s_2',x)$ is expressed as a function of collision rate coefficient ($C$) and probability that collision results in agglomeration($\psi$) \citep{ingram2005} –
\begin{align}
\beta(s_1,s_2,s_1',s_2',x) = \beta_oC(s_1,s_2,s_1',s_2')\psi(s_1,s_2,s_1',s_2',x)
%\beta(s_1,s_2,s_1',s_2',x) = & \beta_o*(V(s_1,s_2,x)+V(s_1',s_2',x))^{\gamma}*(c(s_1,s_2,x)\notag\\
%&+c(s_1',s_2',x))^{\alpha}\left(1-\frac{(c(s_1,s_2,x)+c(s_1',s_2',x))^{\delta}}{2}\right)^{\alpha}
\label{eqn:mthds_pbm_beta_kernal}
\end{align}
%\par \textcolor{red}{citation?}

where, $\beta_o$ is aggregation rate constant.
\par Collision rate coefficient ($C$) is a function of particle sizes and is calculated by normalizing the number of collisions between group of particles \citep{gantt2006} and is obtained from LIGGGHTS DEM simulation. A recent study shows that collision frequency depends on PSD as well \citep{sen2014}. Collision rate coefficient can be expressed as:
\begin{align}
C(s_1,s_2,s_1',s_2')=\frac{N_{coll}(s_1,s_2,s_1',s_2')}{N_p(s_1,s_2)N_p(s_1',s_2')\Delta t}
\label{eqn:collfreq}
\end{align}
\par In equation(\ref{eqn:collfreq}), $N_{coll}$ is the number of collision between two solid particles in time interval $\Delta t$ \& $N_p$ is number of particle of particular size. The $\psi$ in equation (\ref{eqn:mthds_pbm_beta_kernal}) can be expressed as 

\begin{align}
\psi((s_1,s_2,s_1',s_2') = 
\left\{\begin{matrix}
\psi_0,\hspace{0.2cm} LC((s_1,s_2) \geq LC_{min}\hspace{0.2cm} or\hspace{0.2cm} LC((s_1',s_2') \geq LC_{min}	\\ 
0,\hspace{0.2cm} LC((s_1,s_2) < LC_{min}\hspace{0.2cm} or\hspace{0.2cm} LC((s_1',s_2') < LC_{min}
\end{matrix}\right.
\label{eqn:colleff}
\end{align}
\par In equation \ref{eqn:colleff}, $LC$ is the liquid content of particles and $LC_{min}$ stands for minimum liquid content required for coalescence of particle. 

%%%%% commented the breakage details as we aren't including in results


%	    \par Similarly, the breakage rate is expressed as-
%	
%	    \begin{align}
%	    \Re_{break}(s_1,s_2,x) = \int_0^{s_{max_1}} \int_0^{s_{max_2}} K_{break}(s_1',s_2',x)F(s_1',s_2',x)ds_1'ds_2' - K_{break}(s_1,s_2,x)F(s_1,s_2,x)
%	    \end{align}
%%	    \par \textcolor{red}{citation?}
%	    
%	    \par where, the breakage kernel $K_{break}(s_1,s_2,x)$ is formulated as – 
%	    
%	    \begin{align}
%	    K_{break}(s_1,s_2,x) = C_{impact}\int_{U_{break}}^{\infty}p(U)dU
%	    %K_{break}(s_1,s_2,x)=\left(\frac{4}{15\pi}\right)^{(\frac{1}{2})}G_{shear}exp\left(-\frac{B}{R(s_1,s_2,x)}\right)
%	    \end{align}
%	    \par \textcolor{red}{citation?}

%\par where, $G_{shear}$ is the shear rate exerted by the impeller on the granules. $R(s_1,s_2,x)$ is the radius of the granule that breaks and $B$ is the breakage kernel constant. $G_shear$ is calculated as $\frac{\nu_{impeller}*D_{impeller}*PI}{60}$ where $\nu_{impeller}$ and $D_{impeller}$ are respectively the rotational speed and diameter of the impeller.
\par The rate of increase of liquid volume of one particle, $\dot{l}_{add}(s_1,s_2,x)$ is expressed as

\begin{align}
\dot{l}_{add}(s_1,s_2,x) = \frac{(s_1+s_2)(\dot{m}_{spray}(1-c_{binder})-\dot{m}_evap)}{m_{solid}(x)}
\end{align}

where, $(s_1+s_2)$  is the total solid volume of the particle; $\dot{m}_spray$ is the rate of external liquid addition, $c_{binder}$ is the concentration of binder in the external liquid (which is assumed to be zero in this case as pure liquid is added); $\dot{m}_{evap}$ is the rate of evaporation of liquid from the system (which is also assumed to be zero in this case) and $m_{solid}$ is the total amount of solid present in the compartment.
\par The rate of decrease in gas volume per particle due to consolidation is calculated using the following expression: 

%	      \begin{align}
%	      \dot{g}_{cons}(s_1,s_2,x)=c*(\nu_{impeller})^{\omega}*V(s_1,s_2,x)\frac{(1-\epsilon_{min})}{s} \left[g(s_1,s_2,x)+l(s_1,s_2,x) -(s_1+s_2)\frac{\epsilon_{min}}{1-\epsilon_{min}}\right]
%	      \end{align}  

\begin{align}
\dot{g}_{cons}(s_1,s_2,x)=&c(\nu_{impeller})^{\omega}V(s_1,s_2,x)\frac{(1-\epsilon_{min})}{s}\times \notag \\ 
& \left[g(s_1,s_2,x)+l(s_1,s_2,x) -(s_1+s_2)\frac{\epsilon_{min}}{1-\epsilon_{min}}\right]
\end{align}        

\par where, $c$ and $\omega$ are the consolidation constants; $v_{impeller}$ is the impeller rotational speed; $V(s_1,s_2,x)$ is the volume of particle, $\epsilon_{min}$ is the minimum porosity; $g(s_1,s_2,x)$ and $l(s_1,s_2,x)$ are respectively the gas and liquid volume of the particle.

\par Particle transfer rate, $F_{out}(s_1,s_2,x)$ in Equation \ref{eqn:mthds_pbm_overall} is calculated as:

\begin{align}
F_{out}(s_1,s_2,x) = F(s_1,s_2,x)*\frac{\nu_{compartment(x)}*dt}{d_{compartment}}
\end{align}

where, $\nu_{compartment(x)}$ and $d_{compartment}$ are respectively the average velocity of particles in compartment $x$ and the distance between the mid-points of two adjacent compartment, which is the distance particles have to travel to move to the next spatial compartment. $dt$ is the time-step.
The values of various parameters used in the model are provided in Table \ref{table:mthds_pbm_parameters}.

\begin{table}[!htb]
\caption{Parameters for PBM}
\label{table:mthds_pbm_parameters}
\begin{center}
\begin{tabular}{l|c|c|c}
\hline
\bf{Parameter} &\bf{Symbol} &\bf{Value} &\bf{Units}\\
\hline
Time step & $\delta t$ & $0.5$ & $s$\\
Total granulation time & $T$ & $45, 45$ & $s$\\
Velocity in axial direction & $v_{axial}$ & $1$ & $ms^{-1}$\\
Velocity in radial direction & $v_{radial}$ & $1$ & $ms^{-1}$\\
Aggregation constant & $\beta_0$ & $1\times10^{-9}$ & $-$\\
Initial particle diameter & $R$ & $150$ & $\mu m$\\
Breakage kernel constant & $B$ & $0$ & $-$\\
Diameter of impeller & $D$ & $0.114$ & $m$ \\
Impeller rotation speed & $RPM$ & $2000$ & $rmp$\\
Minimum granule porosity & $\epsilon_{min}$ & $0.2$ & $-$\\
Consolidation rate & $C$ & $0$ & $-$\\
Total starting particles in batch & $F_{initial}$ & $1 \times 10^{6}$ & $-$\\
Liquid to solid ratio & $L/S$ & $0.35$ & $-$ \\
Number of Compartments & $c$ & $4$ & $-$ \\
Number of first solid bins & $s$ & $16$ & $-$\\
Number of second solid bins & $ss$ & $16$ & $-$\\
\hline
\end{tabular}
\end{center}
\end{table}

\subsubsection{Parameters}


\subsection{PBM Parallel C++}
\subsubsection{Discretization \& Parallelizing PBM}
\par The PBM was discretized by converting each of its coordinates in to discrete bins. For the spatial coordinates a linear bin spacing was used. For the internal coordinates, solid,liquid, and gas a nonlinear binning was used. %\textcolor{cyan}{get more details from Anik on this will probably need more detail on binning for reproducability}
\par Once the PBM had been discretized (compartmentalized/binned) a finite differences method was used which created a system of ordinary differential equations (ODEs). The numerical integration technique used to evaluate the system of ODEs was first order Euler integration as it is commonly used to solve these types of systems and author  found it improved speeds while having minimal impact on accuracy. To obtain the most optimal parallel performance, when solving the PBM, work loads were distributed in a manner which took into account the shared memory and distributed memory aspects of the clusters the PBM was being run on. To parallelize the model in a why which could take advantage of shared memory but still effectively run across a distributed system both OMP and MPI were implemented. 
\par One MPI process was used per CPU socket and one OMP process was used per CPU core, as authors (\cite{Bettencourt2017}) found it resulted in the best performance. MPI was used for message passing from one node to another while OMP was used for calculations on each node that could be efficiently solved using a shared memory system  
\par Pseudo code is presented below illustrating how the calculations are distributed and carried out during the simulation. For each time step the MPI processes are made responsible for a specific chunk of the spatial compartments. Then each OMP thread, inside of each MPI process, is allocated to one of the cores of the the multi-core CPU the MPI process is bound too. The OMP processes divide up and compute $\Re_{agg}$ and $\Re_{brk}$. After $\Re_{agg}$ and $\Re_{brk}$ are calculated the MPI processes calculate the new PSD value for their chunk at that specific time step, $F_{t,c}$. The slave processes send their $F_{t,c}$ to the master processes which collects them into the complete $F_{t,all}$. The master process then broadcasts the $F_{t,all}$ value to all slave processes. This decompostion of the data into different CPUs and further into various threads is illustrated in Figure \ref{fig:mthds_PBM_decompostion}.	

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{PBM_decomposition.pdf}
\caption{The distribution of calculations using hybrid parallelisation (MPI + OMP).}
%\caption{hello yuktesh}
\label{fig:mthds_PBM_decompostion}
\end{figure}

\par A crucial feature of the PBM is that the current PSD ($F_{t,all}$) value is used to compute a new time step size for the next iteration. This means all of the MPI processes need to have the same dynamic time step size at each iteration for the calculations to be properly carried out in parallel. Since the completely updated $F_{t,all}$ value is shared before calculating a new time step each process will have the same $F_{t,all}$ value. As a result each process calculates the same size for the new time step. 
%\textcolor{cyan}{ Did not include the liquid and gas PBMs in this but hoping they will be some what assumed? Also the Ragg omp distributed work is an a}
%\textcolor{red}{what about private OMP vars specified that has impact on how model is solved etc. Should look into this. might change based on locking/blocking tests that need to be implemented still.}   

\begin{algorithm}[!h]
\caption*{\bf{Algorithm: }Pseudo code}
\begin{algorithmic}[*]
\While{ $t<t_{final}$ }
\par // the spatial domain is divided into equal chunks (with in 1 bin size)
\par // each MPI process is assigned on chunk of spatial domain shown as $c_{low}$ to $c_{up}$ 
\par // sum all $c_{low_i}$ to $c_{up_i}$ is = to [0,numCompartments]

\For{each MPI processes} $c = c_{low_i}$ to $c_{up_i}$
	\par   // each MPI process is further divided with OMP to take advantage of multi-core CPU
\par   // each OMP process is allocated to a single compute core
\par   // $\Re$ integrals $(i1)$ $\int_{0}^{s_2}$, $(i2)$ $\int_{0}^{s_{max_2}-s_2}$, and $(i3)$ $\int_{0}^{s_{max_2}-s_2}$ are divided into smaller integrals
\par  // $\int_{i_1low_n}^{i_1up_n}$, $\int_{i_2low_n}^{i_2up_n}$, and $\int_{i_3low_n}^{i_3up_n}$ which are solved by the "n" OMP processes
\par   // allocated to that MPI process (CPU)
  \For{ each OMP process}

\begin{align}
\Re_{agg}(s_1,s_2,c)=& \frac{1}{2}\int_{0}^{s_{1}} \int_{i_1low_n}^{i_1up_n} \beta(s_1',s_2',s_1-s_1',s_2-s_2',c)F(s_1',s_2',c)F(s_1-s_1',s_2-s_2',c)ds_1'ds_2'\notag\\ 
&- F(s_1,s_2,c)\int _{0}^{s_{max_1}-s_1} \int_{i_2low_n}^{i_2up_n} \beta(s_1,s_2,s_1',s_2',c)F(s_1',s_2',c)ds_1'ds_2'\notag
\end{align}
\begin{align*}
\Re_{break}(s_1,s_2,c) = \int_{0}^{s_{max_1}} \int_{i_3low_n}^{i_3up_n} K_{break}(s_1',s_2',c)F(s_1',s_2',c)ds_1'ds_2' - K_{break}(s_1,s_2,c)F(s_1,s_2,c)
\end{align*}
 \EndFor 
\begin{align*}
F_{t,c} &= \frac{\Delta F(s_1,s_2,c)}{\Delta t}\Delta t  + F(s_1,s_2,c)_{t-1}\\
          &=   (\Re_{agg}(s_1,s_2,c)+\Re_{break}(s_1,s_2,c)+\dot{F}_{in}(s_1,s_2,c)-\dot{F}_{out}(s_1,s_2,c))\Delta t + F(s_1,s_2,c)_{t-1}
\end{align*}

\EndFor
\State MPI Send $F_{t,c}$ to Master MPI process
\State MPI Recv $F_{t,c}$ from MPI all slave processes
\State Master consolidate all $F_{t,c}$ chunks into a complete $F_{t,all}$
\State Master does inter-bin particle transfers (updates $F_{t,all}$)
\State MPI Bcast $F_{t,all}$ to all slave processes
\State $t_{new} = t + timestep$

\EndWhile   

\end{algorithmic}
\end{algorithm}   


\subsection{RP \& PBM+DEM Communication}


A primary challenge faced is the scalable execution of multiple (often two,
but possibly more) heterogeneous simulations that need to run independently
but have a need to communicate and exchange information. Traditionally each
simulation is submitted as an individual job, but that brings invariably leads
to the situation where each simulations gets through the batch-queue systems
independent of the other. So although the first-through-the-queue is ready to
run, it stalls fairly soon waiting for the other simulation to make it through
the queue.  On the other hand MPI capabilities can used to  execute both
simulations as part of a single multi-node job.  Thus whereas the former
method suffers from unpredictable queue time for each job, the latter is
suitable to execute tasks that are homogeneous and have no dependencies, and
relies on the fault tolerance of MPI which are inadequate.


The Pilot abstraction~\cite{review_pilotreview} solves these issues:  The
pilot abstraction, (i) uses a container-job as a placeholder to acquire
resources via the local resource management system (LRMS) and,  (ii) to
decouple the initial resource acquisition from task-to-resource assignment.
Once the pilot (container-job) is scheduled via the LRMS, it can then directly
be populated with the computational tasks. This functionality allows all the
computational tasks to be executed directly on the resources, without being
queued at the LRMS individually. This approach thus supports the requirements
of task-level parallelism and high-throughput as needed by science drivers.

RADICAL-Pilot is an implementation of the pilot abstraction, engineered to
support scalable and efficient launching of heterogeneous tasks
across different platforms.



\section{Results}

\subsection{DEM}
\subsubsection{DEM Spacial Decomposition Studies}
\par LIGGGHTS as discussed above, statically decomposes the work space and each section is sent to a MPI process for calculations. Thus, the division of the space becomes an important criteria for the simulation for efficient load balancing. The initial studies were undertaken for a mono-sized particle of size 1 mm and the simulation was carried out for 0.5 seconds of wall time. The initial timing studies for the decomposition were performed using 64 cores. The effect of the decomposition on the simulation time can be seen in Table \ref{table:rslts_dem_slicing_studies}. This indicates that dividing the x-direction in more number of compartments help increase the speed of the simulation. This is easy to comprehend since the granulator has its length parallel to the x-axis. These results also show that if the geometry is divided into more than 2 compartments in the y-axis or the z-axis the simulation time increases. This can be accounted to the increased communication required to transfer the rotating impeller mesh from one compartment in the y-axis or the z-axis to the another compartment for each time step. Since MPI is limited by communication in between the nodes, a speed decrease is observed due to increased partitioning in these directions.

\begin{table}[ht]
\caption{The effect of spatial decomposition on the performance of the DEM simulations}
\label{table:rslts_dem_slicing_studies}
\begin{center}
\begin{tabulary}{\linewidth}{C|C|C|C}
	  
\hline
\bf{Slices in x-direction}&\bf{Slices in y-direction}&\bf{Slices in z-direction}& \bf{Time taken for a 0.5 second simulation (in minutes)}\\
\hline
$64$ & $1$ & $1$ & $10.27$\\
$32$ & $2$ & $1$ & $8.7$\\
$16$ & $2$ & $2$ & $6.83$\\
$8$ & $4$ & $2$ & $7.2$\\		  
$8$ & $2$ & $4$ & $7.96$\\
\hline  		  
\end{tabulary}
\end{center}
	      
\end{table}

\par When MPI is used for parallelization of a task, load balancing becomes an important parameter that needs to be considered. Each processor used during the task is utilised with the similar usage as compared to other processors being used. Following the results from the initial timing studies,the y and the z-axes were not divided in more than 2 compartments for 128 and 256 core simulation as well. This meant that the x-axis was divided into 32 and 64 compartments respectively. In order to avoid the expensive communication between the processes, LIGGGHTS tries to insert the particles towards the centre of the compartment such that the number of ghost atoms are minimized. But, slicing in the x-axis reduced the space available for the insertion of the particles thus, many of the particles were inserted incorrectly. Another abnormal behaviour observed during these simulation was the particles halted at certain compartment and no particle travelled beyond this compartment in the x-direction. Thus, another set of timing studies were performed for the 128 and 256 core simulations. The comparison of simulation times have been shown in table .

\begin{table}[ht]
\caption{Spatial decomposition  of the DEM simulations for higher core counts}
\label{table:rslts_dem_128_256_decomp}
\begin{center}
\begin{tabulary}{\linewidth}{C|C|C|C|C}
	  
\hline
\bf{Number of cores used}&\bf{Slices in x-direction}&\bf{Slices in y-direction}&\bf{Slices in z-direction}& \bf{Time taken for a 10 second simulation (in minutes)}\\
\hline
$128$ & $16$ & $2$ & $4$ & $264.67$\\
$128$ & $16$ & $4$ & $2$ & $247.2$\\
$256$ & $16$ & $2$ & $8$ & $271.5$\\		  
$256$ & $16$ & $8$ & $2$ & $252$\\
$256$ & $16$ & $4$ & $4$ & $265.32$\\
\hline  		  
\end{tabulary}
\end{center}
	      
\end{table}
\par The simulation times illustrated in Table \ref{table:rslts_dem_128_256_decomp} shows that incorrectly slicing the geometry also affects the perforance of the system. It can be noted that slicing along y-axis is more favourable than slicing along the z-direction. The insertion of particles is hindered when the geometry is cut along the z-axis as inlet is perpendicular to it. LIGGGHTS thus provisions lesser space for the insertion of these particles, thus increasing the time of the simulation. Thus, just slicing the geometry into 2 sections along the z-axis is preferred. So, the chosen configurations for the final simulations consisted of the geometry having only 2 sections in the z-direction and more slices along the y-axis. 

\subsubsection{DEM Performance}
\par A test case was run for mono-sized particle of 2mm diameter for timing comparison studies. The times are plotted in figure \ref{fig:rslts_DEM_2mm_timing} indicate that using lower number of cores is not feasible for long simulations since the time taken while using 1 core is about $11$x times slower.
Thus, the simulations were carried out in core configurations of 64, 128 and 256 cores. The studies undertaken had 5 mono-sized population of particles of diameter 0.63, 1, 1.26, 1.59 and 2 mm simulations and one simulation consisted of particle size distribution. Figure \ref{fig:rslts_DEM_timing_studies} shows that the amount of CPU time required for a 10 second simulation of the granulator. The post processing MATLAB script was run on the dump files obtained from the simulation and it was observed that the system reached a steady state about 3-5 seconds of the simulation time. Particles with larger diameter reached steady state at a faster rate with an average hold-up of particles of about 6500. 
\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{rslts_DEM_2mm_timing.pdf}
\caption{The variation in the amount of time taken for the simulation as a function of \# of cores}
%\caption{hello tesh}
\label{fig:rslts_DEM_2mm_timing}
\end{figure}	    
\par The figure \ref{fig:rslts_DEM_timing_studies} indicates that there is not a significant amount of speed improvment when 256 cores are used for the simulation over 128 cores. This speed decrease can be accounted to the communication time between the MPI processes. When the particles move from one section to another of the space, they are transfered as ghost particles from one process to another process. Thus, there is large amount of communication which is required. One of the issues of using a cluster with shared memory within the nodes but none in between nodes is that it has to rely on the netwrking infrastructure of the cluster which bottlenecks the communications thus leading to higher communication times. There are more sections present when 256 cores are used for the simulation, thus there is more communication in this system when compared to 128 core simulation. This excess communication makes the simulation slower though there is more processing power and it requires lesser time for other calculations. Another observation that can be made from the Figure \ref{fig:rslts_DEM_timing_studies} is that the particle size distribution simulation takes more time compared to the simulations with mono-sized particles of 1.59mm and 2mm, though the mean size of particles in the distribtion is 2mm. The default profile provided by LIGGGHTS indicates that the time spent in calculating the particle-particle interaction forces was higher than the mono-sized simulations. The different diameters make the interaction forces more tedious thus, making it computationally more expensive. 

\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{rslts_DEM_timing_plots.pdf}
\caption{Time taken for a 10 second DEM simulation for diameters ranging from 0.63mm to 2mm and a particle size distribution ranging from 1mm - 3mm}
%\caption{hello tesh}
\label{fig:rslts_DEM_timing_studies}
\end{figure}
\par The communication time in LIGGGHTS is indicated by the Modify time, which is the sum of the times required to transfer the rotating mesh from one process to the other. From the figure \ref{fig:rslts_DEM_percent_plot}, it can be seen the major chunk of the simulation time is taken up by the modify time. This is expected since the impeller is rotating at a very high speed of 2000 rpm. So, if the number of processes are increased the amount of time spent in transferring the mesh also increases. In the studies, the modify time as a percentage of the simulation increased from 82\% to about 90\% , when the core count was increased from the 64 cores to 256 cores. But, the using the higher number of cores reduces time taken to calcualte particle-particle interaction as well as in neighbour calculation. Thus, a better implementation for meshing as well as decomposition of the geometry for faster simulations with higher core counts.

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{rslts_DEM_percent_plot.pdf}
\caption{The variation in the amount of time taken for the simulation as a function of \# of cores}
\label{fig:rslts_DEM_percent_plot}
\end{figure}


\subsection{PBM}
\subsubsection{PBM Model Validation}
The Population Balance Model implemented was considered to have an inlet flow of particles in the first compartment at a constant mass flow rate of 15 kilograms per hour. The particles were introduced in a log normal distribution with minimum diameter of the particles being about 12 $\mu$m and the maximum of 2.1 mm. \\
%Since such a large range of diameters are being covered, the number of particles of the smaller bins (Bins 1 - 4) of the granulator will be higher than the number of particles in the larger bin sizes. \\ 
The PBM used in this study employs an aggregation kernel that takes in account the formation of a larger partcle from only two smaller particles. The ratio of the rate of formation and the rate of depletion due to aggregation helps us monitor whether the PBM is running correctly. Since this PBM takes into account only 2 particles at once, the ratio is expected to have a value of 0.5 during the aggregation process. Figure \ref{fig:rslts_PBM_ratio_plot_2mm} illustrates this ratio to be 0.5 throughout the simulation, validating the stability of the PBM used.\\
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{rslts_PBM_2mm_validation.pdf}
\caption{The ratio of rate of formation to the rate of depletion due to aggraetion as a function of time. The constant value of 0.5 shows that the PBM is stable through the simulation time}
\label{fig:rslts_PBM_ratio_plot_2mm}
\end{center}
\end{figure}
Figure \ref{fig:rslts_PBM_d50_plots} shows four different d\textsubscript{50} plots at four different time instants.Figure \ref{fig:rslts_PBM_d50_plots}(\subref{fig:30s}) shows the distribution of the particles at 30 seconds, where we expect to have the highest number of the smaller particles. Since the degree of aggregation that has occured is very low, thus there is a spike in the number of particels of the smaller size. The particle size distributions at 2 intermmediate times of 50 seconds and 75 seconds are plotted in Figures \ref{fig:rslts_PBM_d50_plots}(\subref{fig:50s}) and \ref{fig:rslts_PBM_d50_plots}(\subref{fig:75s}) respectively. These illustrate that there is an increase in the number of particles with higher diameter and a subsequent decrease in the number of particles for lower diameter particles. Figure \ref{fig:rslts_PBM_d50_plots}(\subref{fig:100s}) shows the distribution of the particle size at the end of the granulation process. It can be seen that the number of particles in the higher diameter bins increase for both the solids. 

\begin{figure}[H]
\begin{subfigure}{.5\textwidth}
\includegraphics[scale=0.45]{rslts:PBM_30s_psd.pdf}
	\caption{•}	
	\label{fig:30s}
\end{subfigure}\hfill
\begin{subfigure}{.5\textwidth}
	
\includegraphics[scale=0.6]{rslts:PBM_50s_psd.pdf}
	\caption{•}
	\label{fig:50s}
\end{subfigure}
	\begin{subfigure}{.5\textwidth}
	
\includegraphics[scale=0.45]{rslts:PBM_75s_psd.pdf}
\caption{•}
	\label{fig:75s}
\end{subfigure}\hfill
	\begin{subfigure}{.5\textwidth}
	
\includegraphics[scale=0.45]{rslts:PBM_100s_psd.pdf}
\caption{•}
	\label{fig:100s}
\end{subfigure}
\caption{The plots for the number of particles of solid 1 and solid 2 and with their respective d50 in mm after (\subref{fig:30s}) 30s (\subref{fig:50s}) 50s (\subref{fig:75s}) 75s and (\subref{fig:100s}) 100s}
\label{fig:rslts_PBM_d50_plots}
\end{figure}   

\subsubsection{Parallel PBM performance studies}
\par The PBM was run for the results obtained from each of the aforementioned DEM simulations. Since the PBM has been parallelised using hybrid techniques, a combination of MPI and OMP cores were used to perform the simulations. Figure \ref{fig:rslts_PBM_timing_studies} shows the average time taken by a PBM simulation for simulation a total of 100 seconds of the process, which included 25 seconds of the mixing time and 75 seconds of granulation time. The time taken for simulating all DEM scenarios by a single set of core configuration of in less than 10\% of each other, thus, an  average time for a single core configuration was used to illustate the performance. Figure \ref{fig:rslts_PBM_timing_studies} shows that the program scales really well to about 32 cores but then, the improvement in the performance is negligible. The times represented have the core configurations of only MPI up till 16 cores after which 32, 64 and 128 core simulations employed 8 OMP threads each. The scaling with the only MPI cores shows great increase in performance. \\
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{rslts_PBM_Time_analysis.pdf}
\caption{Average time taken to run the PBM simulation which consisted of 25 seconds of mixing time and 75 seconds of granulation time at different core configurations}
%\caption{hello tesh}
\label{fig:rslts_PBM_timing_studies}
\end{figure}

Pure timing studies are not really a good measure to represent the parallel performance of a program. Speedup of a parallle program indicates the speed increase of the program when it is run on more compute cores compared to the wall clock time when it is run in serial. It is the most common way to represnt the parallel performance. Speedup is the ratio of the time taken to run the program in serial to the time taken by the program to run in parallel as shown in Equation \ref{eqn:rslts_PBM_Speed_Up}. For an ideally parallelised program, the speedup is 'n' times, where n is the number of cores used.\\

\begin{align}
\textit{Speedup} = \frac{\textit{Serial Wall Clock Time}}{\textit{Parallel Wall Clock Time}}
\label{eqn:rslts_PBM_Speed_Up}
\end{align}

Speedup does not take into account number of processors used in the simulation, thus another metric that is used to determine the s parallel performance is parallel efficiency. This metric is nothig by speedup divided by the number of cores used. Thus, parallel efficiency normalizes speedup and gives a fractional value of the ideal speedup a program achieves with the increase in the number of cores.\\

\begin{align}
\textit{Speedup} = \frac{\textit{Serial Wall Clock Time}}{\textit{Parallel Wall Clock Time $\times$ $n_{cores}$}}
\label{eqn:rslts_PBM_parallel_efficiency}
\end{align}
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{rslts_PBM_Speed_Up.pdf}
\caption{Average time taken to run the PBM simulation which consisted of 25 seconds of mixing time and 75 seconds of granulation time at different core configurations}
%\caption{hello tesh}
\label{fig:rslts_PBM_speed_up}
\end{figure}
\begin{figure}[H]
\centering
\includegraphics[scale=0.7]{rslts_PBM_Efficiency.pdf}
\caption{Average time taken to run the PBM simulation which consisted of 25 seconds of mixing time and 75 seconds of granulation time at different core configurations}
%\caption{hello tesh}
\label{fig:rslts_PBM_parallel_efficiency}
\end{figure}


\subsection{PBM+DEM - RP} 
\subsubsection{PBM+DEM Validation/Accuracy?}
%	   	\par Talk about how the physics remain the same whether mono-sized particles are used or a size distribution is used.
%	   \par Figs comparing the one-way coupling from both these sources.
%	    
\subsubsection{PBM+DEM Performance}
%	    \par strong scaling
%	    \par fig PBM + DEM RP strong scaling 
%	      \begin{figure}[!htb]
%	      \centering
%	      \includegraphics[scale=0.5]{rslts_pbmbyrp_strng}
%	      \caption{PBM+DEM scale with RP}
%	      %\caption{hello tesh}
%	      \label{fig:rslts_dembyRP_strng}
%	      \end{figure}
%	
%\par weak scaling
%\par fig PBM + DEM RP weak scaling


\subsubsection{PBM+DEM Parameter studies}
%	   \par show how PBM+DEM captures multi-physics as parameters changed. helps validate and support model development. show we have made a useful tool for future work.
%	   \par fig 
%	      \begin{figure}[H]
%	      \centering
%	      \includegraphics[scale=0.7]{rslts_psd_evo_time}
%	      \caption{PBM+DEM scale with RP}
%	      %\caption{hello tesh}
%	      \label{fig:rslts_psd_evo_time}
%	      \end{figure}
%	    
\section{Conclusions}

\section*{References} 
\bibliographystyle{elsarticle-harv}
\bibliography{Bibli,radical_publications}
\end{document}
